{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2048\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型配置\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "hidden_size = 128\n",
    "\n",
    "intermediate_size = (int(hidden_size * 8/3 / 128) + 1) * 128\n",
    "\n",
    "\n",
    "config = AutoConfig.for_model(\n",
    "    model_type=\"llama\",\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=2,\n",
    "    num_key_value_heads=4,\n",
    "    tie_word_embeddings=True,vocab_size=2048,max_position_embeddings=512\n",
    ")\n",
    "\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='story_tokenizer_2048', vocab_size=2048, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|start_story|>', 'eos_token': '<|end_story|>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<|start_story|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"<|end_story|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('story_tokenizer_2048')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name & Parameters\n",
      "----------------------------\n",
      "model.embed_tokens.weight                          | Size: torch.Size([2048, 128])        | Count: 262144              \n",
      "model.layers.0.self_attn.q_proj.weight             | Size: torch.Size([128, 128])         | Count: 16384               \n",
      "model.layers.0.self_attn.k_proj.weight             | Size: torch.Size([64, 128])          | Count: 8192                \n",
      "model.layers.0.self_attn.v_proj.weight             | Size: torch.Size([64, 128])          | Count: 8192                \n",
      "model.layers.0.self_attn.o_proj.weight             | Size: torch.Size([128, 128])         | Count: 16384               \n",
      "model.layers.0.mlp.gate_proj.weight                | Size: torch.Size([384, 128])         | Count: 49152               \n",
      "model.layers.0.mlp.up_proj.weight                  | Size: torch.Size([384, 128])         | Count: 49152               \n",
      "model.layers.0.mlp.down_proj.weight                | Size: torch.Size([128, 384])         | Count: 49152               \n",
      "model.layers.0.input_layernorm.weight              | Size: torch.Size([128])              | Count: 128                 \n",
      "model.layers.0.post_attention_layernorm.weight     | Size: torch.Size([128])              | Count: 128                 \n",
      "model.layers.1.self_attn.q_proj.weight             | Size: torch.Size([128, 128])         | Count: 16384               \n",
      "model.layers.1.self_attn.k_proj.weight             | Size: torch.Size([64, 128])          | Count: 8192                \n",
      "model.layers.1.self_attn.v_proj.weight             | Size: torch.Size([64, 128])          | Count: 8192                \n",
      "model.layers.1.self_attn.o_proj.weight             | Size: torch.Size([128, 128])         | Count: 16384               \n",
      "model.layers.1.mlp.gate_proj.weight                | Size: torch.Size([384, 128])         | Count: 49152               \n",
      "model.layers.1.mlp.up_proj.weight                  | Size: torch.Size([384, 128])         | Count: 49152               \n",
      "model.layers.1.mlp.down_proj.weight                | Size: torch.Size([128, 384])         | Count: 49152               \n",
      "model.layers.1.input_layernorm.weight              | Size: torch.Size([128])              | Count: 128                 \n",
      "model.layers.1.post_attention_layernorm.weight     | Size: torch.Size([128])              | Count: 128                 \n",
      "model.norm.weight                                  | Size: torch.Size([128])              | Count: 128                 \n",
      "----------------------------\n",
      "Total Parameters: 656000 (0.7 M)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir='saves',\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=False,\n",
    "        eval_steps=1000,\n",
    "        per_device_train_batch_size=64,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=5e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        bf16=True,\n",
    "        logging_steps=25, \n",
    "        report_to=\"wandb\",\n",
    "        num_train_epochs=2,\n",
    "        save_steps=10000000,\n",
    "        seed=3407,\n",
    "        warmup_steps=800\n",
    "    )\n",
    "def init_model():\n",
    "    model = AutoModelForCausalLM.from_config(                    \n",
    "        config,\n",
    "        torch_dtype=torch.float32   # 全精度训练\n",
    "    ).to(device)                    # 迁移到 device 上\n",
    "\n",
    "    # Kaiming 初始化\n",
    "    def kaiming_initialization(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            elif 'bias' in name:\n",
    "                # 一般偏置项可以初始化为 0\n",
    "                torch.nn.init.constant_(param, 0)\n",
    "\n",
    "    kaiming_initialization(model)\n",
    "\n",
    "    def print_model_parameters(model):\n",
    "        print(\"Layer Name & Parameters\")\n",
    "        print(\"----------------------------\")\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            param_size = parameter.size()\n",
    "            param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "            total_params += param_count\n",
    "            print(f\"{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}\")\n",
    "        print(\"----------------------------\")\n",
    "        print(f\"Total Parameters: {total_params} ({total_params / 1000000:.1f} M)\")\n",
    "\n",
    "    print_model_parameters(model)\n",
    "    return model\n",
    "model=init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    input_text: str = \"Once upon a time, \",\n",
    "    max_new_tokens: int = 16\n",
    "):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(generated_text)\n",
    "\n",
    "# 测试推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = AutoModelForCausalLM.from_config(                    \n",
    "        config,\n",
    "        torch_dtype=torch.float32   # 全精度训练\n",
    "    ).to(device)                    # 迁移到 device 上\n",
    "\n",
    "    # Kaiming 初始化\n",
    "    def kaiming_initialization(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            elif 'bias' in name:\n",
    "                # 一般偏置项可以初始化为 0\n",
    "                torch.nn.init.constant_(param, 0)\n",
    "\n",
    "    kaiming_initialization(model)\n",
    "\n",
    "    def print_model_parameters(model):\n",
    "        print(\"Layer Name & Parameters\")\n",
    "        print(\"----------------------------\")\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            param_size = parameter.size()\n",
    "            param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "            total_params += param_count\n",
    "            print(f\"{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}\")\n",
    "        print(\"----------------------------\")\n",
    "        print(f\"Total Parameters: {total_params} ({total_params / 1000000:.1f} M)\")\n",
    "\n",
    "    print_model_parameters(model)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name_or_path = \"TinyStoriesV2_SpecialTokens\"        # 可以替换为本地文件夹路径\n",
    "\n",
    "# ds_train = load_dataset(dataset_name_or_path, split='train')        # 取全部数据\n",
    "ds_train = load_dataset(dataset_name_or_path, split='train[:10%]')\n",
    "\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def process_func(\n",
    "    examples: Dict[str, List]\n",
    ") -> Dict[str, List]:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('story_tokenizer_2048')\n",
    "    max_token = 512    # 设置最长 token 数目，对于我们当前任务，2048 绝对不会超\n",
    "\n",
    "    encoded_texts = tokenizer(examples['text'], add_special_tokens=False)\n",
    "    input_ids_list = encoded_texts['input_ids']\n",
    "\n",
    "    new_input_ids_list, new_attn_mask_list = [], []\n",
    "    for input_ids in input_ids_list:\n",
    "        temp = input_ids[-max_token+1:] + [tokenizer.eos_token_id]\n",
    "        new_input_ids_list.append(temp)\n",
    "        new_attn_mask_list.append([1] * len(temp))\n",
    "    return {\n",
    "        \"input_ids\": new_input_ids_list,\n",
    "        \"attention_mask\": new_attn_mask_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 16                                    # 处理数据时所用的线程数\n",
    "\n",
    "ds_train = ds_train.shuffle()\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    process_func,\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=ds_train.column_names,\n",
    "    desc='Running tokenizer on train_set: '\n",
    ")\n",
    "\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "wandb.init()\n",
    "\n",
    "trainer=Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_train,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"<|start_story|>Once upon a time, there was a little boy named Tom. Tom \",\n",
    "    max_new_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_function(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        # 从 wandb 获取超参数\n",
    "        config = wandb.config\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='saves',                         # 输出路径，包括模型检查点、中间文件等\n",
    "        overwrite_output_dir=True,                  # 是否覆写 output_dir\n",
    "        do_train=True,                              # 是否做训练\n",
    "        do_eval=False,                               # 是否做评估\n",
    "        eval_steps=1000,                            # 评估步骤间隔\n",
    "        per_device_train_batch_size=config.batch_size,              # 每设备批次\n",
    "        gradient_accumulation_steps=1,              # 梯度累计步大小，省显存，但小模型没必要，用 1 收敛比较快\n",
    "        learning_rate=config.learning_rate,                         # 学习率大小\n",
    "        lr_scheduler_type='cosine',                 # 学习率调度策略，LLM 训练一般都用余弦\n",
    "        bf16=True,\n",
    "        logging_steps=50,                           # 打印步骤间隔\n",
    "        report_to=\"wandb\",                             # 日志输出目标，不想用 wandb 可以设置为 None\n",
    "        num_train_epochs=1,                         # 训练轮数，2 ~ 3 即可\n",
    "        save_steps=100000,                            # 检查点保存步骤间隔\n",
    "        seed=3407,\n",
    "        warmup_steps=config.warmup_steps\n",
    "    )\n",
    "\n",
    "        trainer=Trainer(\n",
    "            model=init_model(),\n",
    "            args=training_args,\n",
    "            train_dataset=ds_train,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "import os\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # 使用贝叶斯优化\n",
    "    \"metric\": {\n",
    "        \"name\": \"train/loss\",  # 优化的指标是训练 loss\n",
    "        \"goal\": \"minimize\",  # 目标是最小化训练 loss\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 5e-5,\n",
    "            \"max\": 1e-3,\n",
    "            \"distribution\": \"log_uniform_values\",  # 学习率的分布是对数均匀分布\n",
    "        },\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"warmup_steps\": {\"min\": 0, \"max\": 100}\n",
    "    },\n",
    "}\n",
    "os.environ[\"WANDB_TIMEOUT\"] = \"60\"\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"tinystories-lm-1\")\n",
    "wandb.agent(sweep_id, train_function, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '111'\n",
    "\n",
    "model.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
